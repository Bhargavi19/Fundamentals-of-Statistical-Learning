{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling a  handwritten digit classifier using Multi-layer Neural Network. The model will be trained to classify the images of handwritten digits into 10 classes( digits 0 -9).\n",
    "We will be using [MNIST dataset of handwritten digits](http://yann.lecun.com/exdb/mnist/) for training the classifier.The dataset is a good example of real-world data and is widely used by Machine Learning community for learning techniques and pattern recognition methods.\n",
    "\n",
    "MNIST dataset contains grayscale samples of handwritten digits of size 28 $\\times$ 28. It is split into training set of 60,000 examples, and a test set of 10,000 examples. For this assignment, we will using a smaller subset of 1500 training samples, 500 validation samples and 1000 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization\n",
    "from datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import pytest\n",
    "import random\n",
    "import numpy.matlib \n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "train_samples = 1500\n",
    "val_samples = 500\n",
    "test_samples = 1000\n",
    "\n",
    "digits = list(range(10))\n",
    "trX, trY, tsX, tsY, valX, valY = mnist(train_samples,val_samples,test_samples, digits=digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Parameter Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us now define a function that can initialize the parameters of the Neural Network.\n",
    "The network parameters are wrapped as dictionary elements that can easily be passed as function parameters while calculating gradients during back propogation.\n",
    "\n",
    "1. The weight matrix is initialized with random values from a normal distribution of variance $1$. For example, to create a matrix $W$ of dimension $3 \\times 4$, with values from a normal distribution with variance $1$,\n",
    "we define $W = np.random.normal(size =(3,4))$.\n",
    "\n",
    "2. Bias values are initialized with a vector of 0's.\n",
    "\n",
    "The dimension of weight matrix for a layer $(l+1)$ is given by ( Number of neurons in $(l+1)$  X  Number of neurons in $l$ )\n",
    "\n",
    "The dimension of bias vector for a layer $(l+1)$ is given by ( Number of neurons in $(l+1)$  X  Number of neurons in $1$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(net_dims):\n",
    "    numLayers = len(net_dims)\n",
    "    parameters = {}\n",
    "    for l in range(numLayers-1):    \n",
    "        parameters[\"W\"+str(l+1)] = np.random.normal(size=(net_dims[l+1], net_dims[l]))\n",
    "        parameters[\"b\"+str(l+1)] = np.zeros(net_dims[l+1]).reshape(net_dims[l+1], 1)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Activation function takes an input from the previous layer and performs a certain fixed mathematical operation and the result is passed to the following layer.\n",
    "1. ReLU or Rectified Linear Unit\n",
    "ReLU (Rectified Linear Unit) is a piecewise linear function that will output the input if is positive, otherwise, it's output is zero.\n",
    "\\begin{equation*}\n",
    "ReLU(x) = Max(0,x)\n",
    "\\end{equation*}\n",
    "2. Linear activation\n",
    " Linear activation performs a simple linear operation of passing the input.\n",
    "\\begin{equation*}\n",
    "Linear(x) = x\\\\\n",
    "dx = 1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    cache = {}\n",
    "    features = Z.shape[0]\n",
    "    samples = Z.shape[1]\n",
    "    A = np.zeros((features, samples))\n",
    "    for i in range(features) :\n",
    "        for j in range(samples) :\n",
    "            A[i][j] = np.maximum(0, Z[i][j])\n",
    "    cache[\"Z\"] = Z\n",
    "    return A, cache      \n",
    "\n",
    "def relu_der(dA, cache):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    Z = cache[\"Z\"]\n",
    "    features = dZ.shape[0]\n",
    "    samples = dZ.shape[1]\n",
    "    for i in range(features) :\n",
    "        for j in range(samples) :\n",
    "            dZ[i][j] = np.maximum(0, dZ[i][j])\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(Z):\n",
    "    A = Z\n",
    "    cache = {}\n",
    "    cache[\"Z\"] = Z\n",
    "    return A, cache\n",
    "\n",
    "def linear_der(dA, cache):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "The softmax activation is computed on the outputs from the last layer and the output label with the maximum probablity is predicted as class label. The softmax function can also be refered as  normalized exponential function which takes a vector of $n$ real numbers as input, and normalizes it into a probability distribution consisting of $n$ probabilities proportional to the exponentials of the input numbers. \n",
    "\n",
    "The input to the softmax function is the matrix of all the samples, $ Z = [ z^{(1)} , z^{(2)}, \\ldots, z^{(m)} ] $, where $z^{(i)}$ is the $i^{th}$ sample of $n$ dimensions. We estimate the softmax for each of the samples $1$ to $m$. The softmax activation for $a^{(i)} = \\text{softmax}(z^{(i)})$ is, \n",
    "\\begin{equation}\n",
    "a_k{(i)} = \\frac{exp(z^{(i)}_k)}{\\sum_{k = 1}^{n}exp(z^{(i)}_k)} \\qquad \\text{for} \\quad 1\\leq k\\leq n\n",
    "\\end{equation}\n",
    "\n",
    "The output of the softmax is $ A = [ a^{(1)} , a^{(2)} .... a^{(m)} ]$, where $a^{(i)} = [a^{(i)}_1,a^{(i)}_2, \\ldots, a^{(i)}_n]^\\top$.  In order to avoid floating point overflow, we subtract a constant from all the input components of $z^{(i)}$ before calculating the softmax. This constant is $z_{max}$, where, $z_{max} = max(z_1,z_2,...z_n)$. \n",
    "Note: \n",
    "\\begin{equation}\n",
    "a_k{(i)} = \\frac{exp(z^{(i)}_k- z_{max})}{\\sum_{k = 1}^{n}exp(z^{(i)}_k - z_{max})} \\qquad \\text{for} \\quad 1\\leq k\\leq n\n",
    "\\end{equation}\n",
    "\n",
    "If the output of softmax is given by $A$ and the ground truth is given by $Y = [ y^{(1)} , y^{(2)}, \\ldots, y^{(m)}]$, the cross entropy loss between the predictions $A$ and groundtruth labels $Y$ is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "Loss(A,Y) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^{n}I \\{ y^i = k \\} \\text{log}a_k^i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $I$ is the identity function given by \n",
    "\n",
    "\\begin{equation}\n",
    "I\\{\\text{condition}\\} = 1, \\quad \\text{if condition = True}\\\\\n",
    "I\\{\\text{condition}\\} = 0, \\quad \\text{if condition = False}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the multiclass cross entropy loss can be given as the difference between the Activation output and ground truth. If $A$ is vector of $m$ samples , as $ A = [ a^{(1)} , a^{(2)} .... a^{(m)} ]$, the gradient of softmax is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "dZ =\\frac{1}{m} (A -Y)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss(Z, Y=np.array([])):\n",
    "    n = Z.shape[0]\n",
    "    m = Z.shape[1]\n",
    "    A = np.zeros((n, m))\n",
    "    zmax = np.max(Z, axis = 0)\n",
    "    D = np.sum(np.exp(Z - zmax), axis=0)\n",
    "    A = np.exp(Z - zmax)/D \n",
    "    loss = []\n",
    "    if len(Y) > 0 : \n",
    "        loss = -1 * np.sum(np.multiply(np.sum(np.log(A)), np.where(A == Y, 1, 0)))/m   \n",
    "    cache = {}\n",
    "    cache[\"A\"] = A\n",
    "    return A, cache, loss\n",
    "\n",
    "def softmax_cross_entropy_loss_der(Y, cache):\n",
    "    A = cache[\"A\"]\n",
    "    m = Y.shape[1]\n",
    "    dZ = (A - Y)/m  \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y,num_classes):\n",
    "    Y_one_hot = np.zeros((num_classes,Y.shape[1]))\n",
    "    for i in range(Y.shape[1]):\n",
    "        Y_one_hot[int(Y[0,i]),i] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_forward(A, drop_prob, mode='train'):        \n",
    "        mask = np.random.rand(*A.shape) > drop_prob\n",
    "        out = None       \n",
    "        if mode == 'train':\n",
    "            out = np.multiply(A, mask)\n",
    "            out = out / (1-drop_prob)\n",
    "        elif mode == 'test':\n",
    "            out = A\n",
    "        else:\n",
    "            raise ValueError(\"Mode value not set, set it to 'train' or 'test'\")\n",
    "        cache = (mode, mask)\n",
    "        return out, cache\n",
    "\n",
    "def dropout_backward(cache, dout):\n",
    "        dA = None\n",
    "        mode, mask = cache\n",
    "        if mode == 'train':\n",
    "            dA = np.multiply(dout, mask)\n",
    "        elif mode == 'test':\n",
    "            dA = dout \n",
    "        else:\n",
    "            raise ValueError(\"Mode value not set, set it to 'train' or 'test'\")\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "One Layer\n",
    "  \n",
    "If the vectorized input to any layer of neural network is $A$ and the parameters of the layer is given by $(W,b)$ ,the output of the layer is,\n",
    "\\begin{equation}\n",
    "Z = W A + b\n",
    "\\end{equation}\n",
    "\n",
    "## Layer + Activation\n",
    "In addition to layer, the following function also computes the activation of each layer which is given by,\n",
    "\\begin{equation}\n",
    "Z = W X + b\\\\\n",
    "A = \\sigma (Z)\n",
    "\\end{equation}\n",
    "\n",
    "depending on the activation choosen for the given layer, the $\\sigma(.)$ can represent different operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = {}\n",
    "    cache[\"A\"] = A\n",
    "    return Z, cache \n",
    "\n",
    "def layer_forward(A_prev, W, b, activation, drop_prob, mode):\n",
    "    Z, lin_cache = linear_forward(A_prev, W, b)    \n",
    "    if activation == \"relu\":\n",
    "        A, act_cache = relu(Z)\n",
    "        A, drop_cache =  dropout_forward(A, drop_prob, mode)        \n",
    "    elif activation == \"linear\":\n",
    "        A, act_cache = linear(Z)\n",
    "        drop_cache = None     \n",
    "    cache = {}\n",
    "    cache[\"lin_cache\"] = lin_cache\n",
    "    cache[\"act_cache\"] = act_cache\n",
    "    cache[\"drop_cache\"] = drop_cache    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layers \n",
    "\n",
    "The forward layers are stacked to form a multi layer network. The number of layers used by the network can be inferred from the size of the $parameters$. If the number of items in the dictionary element $parameters$ is $2L$, then the number of layers will be $L$\n",
    "\n",
    "During forward propagation , the input sample $A_0$ is fed into the first layer and the subsequent layers use the activation output from the previous layer as inputs.\n",
    "\n",
    "Please note all the hidden layers use **ReLU** activation except the last layer which uses **Linear** activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_forward(X, parameters,drop_prob, mode):\n",
    "    L = len(parameters)//2  \n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        A, cache = layer_forward(A, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\", drop_prob, mode)        \n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = layer_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"linear\",drop_prob, mode)\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagagtion (10 Points)\n",
    "\n",
    "Performing back propagation through the layers and calculating the gradients of the network parameters $(dW,db)$.\n",
    "\n",
    "If the derivative of the loss$\\frac{dL}{dZ}$ is given as $dZ$ and network paramerters are given as $(W,b)$, the gradients $(dW,db)$ can be calculated as,\n",
    "\n",
    "\\begin{equation}\n",
    "dA_{prev} = W^T dZ\\\\\n",
    "dW = dZ A^T\\\\\n",
    "db = \\sum_{i=1}^{m} dz^{(i)}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "where $dZ = [dz^{(1)},dz^{(2)}, \\ldots, dz^{(m)}]$ is the column vector of the gradient of loss in the kth layer.\n",
    "\n",
    "#### Layer + Activation\n",
    "\n",
    "In the below function, we also account for the activation while calculating the derivative.\n",
    "We use the derivative functions defined earlier to calculate $(\\frac{dL}{dZ})$ followed by back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, W, b):   \n",
    "    A = cache['A']\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = np.dot(dZ, A.T)\n",
    "    n = dZ.shape[0]\n",
    "    db = np.sum(dZ, axis=1).reshape(n, 1)   \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def layer_backward(dA, cache, W, b, activation):  \n",
    "    lin_cache = cache[\"lin_cache\"]\n",
    "    act_cache = cache[\"act_cache\"]\n",
    "    drop_cache = cache[\"drop_cache\"]\n",
    "    if activation == \"relu\":        \n",
    "        dA = dropout_backward(drop_cache, dA)\n",
    "        dZ = relu_der(dA, act_cache)        \n",
    "    elif activation == \"linear\":        \n",
    "        dZ = linear_der(dA, act_cache)        \n",
    "    dA_prev, dW, db = linear_backward(dZ, lin_cache, W, b)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layers\n",
    "\n",
    "We have defined the required functions to handle back propagation for single layer. Now we will stack the layers together and perform back propagation on the entire network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_backward(dAL, caches, parameters):\n",
    "    L = len(caches) \n",
    "    gradients = {}\n",
    "    dA = dAL\n",
    "    activation = \"linear\"\n",
    "    for l in reversed(range(1,L+1)):\n",
    "        dA, gradients[\"dW\"+str(l)], gradients[\"db\"+str(l)] = \\\n",
    "                    layer_backward(dA, caches[l-1], \\\n",
    "                    parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\\\n",
    "                    activation)\n",
    "        activation = \"relu\"\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Assembling the different parts of forward propagation into a single unit and use the ouput to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, parameters,mode,drop_prob):   \n",
    "    A, caches = multi_layer_forward(X, parameters,drop_prob, mode)\n",
    "    A_softmax, caches, loss = softmax_cross_entropy_loss(A)\n",
    "    m = X.shape[1]\n",
    "    YPred = np.argmax(A_softmax, axis=0).reshape((1,m))\n",
    "    return YPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "A very popular technique that is used along with gradient descent is Momentum. Instead of using only the gradient of the current step to guide the search for minima, momentum also accumulates the gradient of the past steps to determine the direction of descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):    \n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n",
    "            \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter updates\n",
    "\n",
    "The parameter gradients $(dW,db)$ calculated during back propagation are used to update the values of the network parameters.\n",
    "\n",
    "\\begin{equation}\n",
    "V_{t+1} = \\beta  V_{t} +(1-\\beta)\\nabla J(\\theta_t)\\\\\n",
    "\\theta_{t+1} =\\theta_{t} -\\alpha(V_{t+1}), \\quad \\theta \\in \\{ W,b \\}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\alpha$ is the learning rate of the network and $\\beta$ is the momentum parameter . As discussed in the lecture, decay rate is used to adjust the learning rate smoothly across the gradient curve to avoid overshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, gradients, epoch, v, beta, learning_rate, decay_rate=0.01):  \n",
    "    alpha = learning_rate*(1/(1+decay_rate*epoch))\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    for i in range(L):\n",
    "        v[\"dW\"+str(i+1)] = beta * v[\"dW\"+str(i+1)] + (1 - beta) * gradients[\"dW\"+str(i+1)]\n",
    "        v[\"db\"+str(i+1)] = beta * v[\"db\"+str(i+1)] + (1 - beta) * gradients[\"db\"+str(i+1)]\n",
    "        parameters[\"W\"+str(i+1)] = parameters[\"W\"+str(i+1)] - alpha * v[\"dW\"+str(i+1)]\n",
    "        parameters[\"b\"+str(i+1)] = parameters[\"b\"+str(i+1)] - alpha * v[\"db\"+str(i+1)]\n",
    "    return parameters, alpha, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "Assembling all the components of the neural network together and define a complete training loop for the Multi-layer Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_network(X, Y,valX, valY, net_dims, drop_prob, mode, num_iterations=500, learning_rate=0.2, decay_rate=0.00005):\n",
    "\n",
    "    parameters = initialize(net_dims)\n",
    "    A0 = X\n",
    "    costs = []\n",
    "    val_costs = []\n",
    "    num_classes = 10\n",
    "    Y_one_hot = one_hot(Y,num_classes)\n",
    "    valY_one_hot = one_hot(valY,num_classes)\n",
    "    alpha = learning_rate\n",
    "    beta = 0.9\n",
    "    \n",
    "    for ii in range(num_iterations):\n",
    "        Z, cache_1 = multi_layer_forward(A0, parameters, drop_prob, mode)\n",
    "        AL, cache_2, cost = softmax_cross_entropy_loss(Z, Y_one_hot) \n",
    "        dZ = softmax_cross_entropy_loss_der(Y_one_hot, cache_2)\n",
    "        gradients = multi_layer_backward(dZ, cache_1, parameters)\n",
    "        v = initialize_velocity(parameters)\n",
    "        parameters, alpha, v = update_parameters_with_momentum(parameters, gradients, num_iterations, v, beta, learning_rate, decay_rate)                \n",
    "        \n",
    "        Z_, cache_ = multi_layer_forward(valX, parameters, drop_prob, \"test\")\n",
    "        AL, cache_2_, val_cost = softmax_cross_entropy_loss(Z_, valY_one_hot)\n",
    "    \n",
    "        if ii % 10 == 0:\n",
    "            costs.append(cost)\n",
    "            val_costs.append(val_cost)\n",
    "        if ii % 10 == 0:\n",
    "            print(\"Cost at iteration %i is: %.05f, learning rate: %.05f\" %(ii, cost, alpha))\n",
    "    \n",
    "    return costs, val_costs, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1 - Overfittting case, No dropout regularization\n",
    "\n",
    "net_dims = [784,516,256]\n",
    "net_dims.append(10) # Adding the digits layer with dimensionality = 10\n",
    "print(\"Network dimensions are:\" + str(net_dims))\n",
    "\n",
    "# getting the subset dataset from MNIST\n",
    "train_data, train_label, test_data, test_label, val_data, val_label = mnist(noTrSamples=train_samples,noValSamples= val_samples,noTsSamples=test_samples,digits= digits)\n",
    "\n",
    "# initialize learning rate and num_iterations\n",
    "learning_rate = .03\n",
    "num_iterations = 500\n",
    "\n",
    "drop_prob = 0\n",
    "mode = 'train'\n",
    "\n",
    "costs,val_costs, parameters = multi_layer_network(train_data, train_label,val_data, val_label, net_dims, drop_prob, mode, \\\n",
    "        num_iterations=num_iterations, learning_rate= learning_rate)\n",
    "\n",
    "# compute the accuracy for training set and testing set\n",
    "\n",
    "\n",
    "mode ='test'\n",
    "train_Pred = classify(train_data, parameters,mode,drop_prob)\n",
    "val_Pred = classify(val_data, parameters,mode,drop_prob)\n",
    "test_Pred = classify(test_data, parameters,mode,drop_prob)\n",
    "print(train_Pred.shape)\n",
    "\n",
    "\n",
    "trAcc = ( 1 - np.count_nonzero(train_Pred - train_label ) / float(train_Pred.shape[1])) * 100 \n",
    "valAcc = ( 1 - np.count_nonzero(val_Pred - val_label ) / float(val_Pred.shape[1])) * 100 \n",
    "teAcc = ( 1 - np.count_nonzero(test_Pred - test_label ) / float(test_Pred.shape[1]) ) * 100\n",
    "print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
    "print(\"Accuracy for validation set is {0:0.3f} %\".format(valAcc))\n",
    "print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
    "\n",
    "X = range(0,num_iterations,10)\n",
    "plt.plot(X,costs)\n",
    "plt.plot(X,val_costs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 2 - using dropout regularization\n",
    "\n",
    "net_dims = [784,516,256]\n",
    "net_dims.append(10) # Adding the digits layer with dimensionality = 10\n",
    "print(\"Network dimensions are:\" + str(net_dims))\n",
    "\n",
    "# getting the subset dataset from MNIST\n",
    "train_data, train_label, test_data, test_label, val_data, val_label = mnist(noTrSamples=train_samples,noValSamples= val_samples,noTsSamples=test_samples,digits= digits)\n",
    "\n",
    "# initialize learning rate and num_iterations\n",
    "learning_rate = .03\n",
    "num_iterations = 500\n",
    "\n",
    "drop_prob = 0.2\n",
    "mode = 'train'\n",
    "\n",
    "costs,val_costs, parameters = multi_layer_network(train_data, train_label,val_data, val_label, net_dims, drop_prob, mode, \\\n",
    "        num_iterations=num_iterations, learning_rate= learning_rate)\n",
    "\n",
    "# compute the accuracy for training set and testing set\n",
    "mode ='test'\n",
    "train_Pred = classify(train_data, parameters,mode,drop_prob)\n",
    "val_Pred = classify(val_data, parameters,mode,drop_prob)\n",
    "test_Pred = classify(test_data, parameters,mode,drop_prob)\n",
    "print(train_Pred.shape)\n",
    "\n",
    "\n",
    "trAcc = ( 1 - np.count_nonzero(train_Pred - train_label ) / float(train_Pred.shape[1])) * 100 \n",
    "valAcc = ( 1 - np.count_nonzero(val_Pred - val_label ) / float(val_Pred.shape[1])) * 100 \n",
    "teAcc = ( 1 - np.count_nonzero(test_Pred - test_label ) / float(test_Pred.shape[1]) ) * 100\n",
    "print(\"Accuracy for training set is {0:0.3f} %\".format(trAcc))\n",
    "print(\"Accuracy for validation set is {0:0.3f} %\".format(valAcc))\n",
    "print(\"Accuracy for testing set is {0:0.3f} %\".format(teAcc))\n",
    "\n",
    "X = range(0,num_iterations,10)\n",
    "plt.plot(X,costs)\n",
    "plt.plot(X,val_costs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
